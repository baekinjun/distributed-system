3장 쓰기전 로그 

3.1 문제

서버가 데이터 저장에 실패해도 강한 지속성을 보장해야 한다.

3.2 해결책

각각의 상태변화를 명령어로 하드디스크에 저장
단일로그는 각각의 서버 프로세스에 유지관리하고 순차적으로 덧붙여진다.
  순차적으로 단일로그를 사용하면 재시작할때 필요한 로그작업과 이후 이어지는 온라인작업처리가 단순해진다.
  각각의 로그 엔트리에 고유 식별자를 부여 이식별자는 분할로그 로우 워터마크로 로그를 정리 및 특정로그 작업 구현
  로그 갱신은 단일 갱신 큐 로 구현

3.3 구현시 고려사항

로그 파일에 기록한 엔트리를 물리적 매체로에 실제로 저장했는지 확인하는일이 매우 중요

로그를 쓸때마다 플러시를 하면 성능저하 그래서 배치 처리필요

로그를 읽을때 로그엔트리가 손상됐는지 감지하는것이 중요
  해결하기 위해 CRC(데이터 무결성 검사하는 오류코드)레코드와 함께 기록
  로그 엔트리 마커도 사용

단일로그 파일은 관리가 점점 어려워지고 빠르게 저장소 용량을 채울수있다.
  분할로그 로우워터마크 기법을 사용하여 해결

쓰기 전 로그는 덧붙이기 전용이어서 중복엔트리 포함 가능 
  중복을 무시해야한다.
  중복 감지하는 메커니즘이 필요

쓰기전 로그가 안전한 저장소를 전제로 해야한다.
  로그 자체를 잃어버릴수 있음 
  복제로그 패턴 을 사용

3.4 트랜잭션 저장소에서의 활용법

쓰기 전 로그는 트랜잭션 저장소를 구현하는데 자주 사용된다.
  간단하게 지속적으로 원자적 갱신이 이루어지는 방법을 제공 

3.5 이벤트 소싱과의 비교(Axon)

변경사항 로그는 이벤트 소싱의 이벤트로그와 유사

로그 엔트리는 상당히 오랜기간 혹은 무기한 보관

4장 분할 로그

4.1 문제

단일 로그 파일은 점점 커지다 보면 기동할때 이를 읽는 과정에서 성능 병목이 생길수있다.

오래된 로그는 지우지만 거대한 단일 파일을 지우는 작업은 구현하기 쉽지않다.

4.2 해결책

로그를 복수개의 분할로 나누고 특정크기가 되면 롤링한다.

구현시 논리 로그 오프셋을 로그 분할 파일로 쉽게 매핑하는 방법이 필요하다.
  각 로그 분할의 이름을 정해진 방식의 접두사와 기본 오프셋으로 생성 
  각각의 로그 시퀀스 번호를 파일이름과 트랜잭션 오프셋 두 부분으로 나눈다.

읽기 연산
  주어진 오프셋에서 로그분할을 식별하고 후속 로그 분할에서 로그 레코드를 모두 읽는다.

4.3 사례

래프트 , 아파치카프카, nosql 

5장 로우 워터마크 

5.1 문제 

쓰기전 로그는 모든 갱신을 지속성 있는 저장소에 유지 관리, 이 로그는 시간에 따라 무한히 커질수있다.

분할로그는 파일을 작게 유지 하지만 따로 확인하지 않는다면 총 디스크 사용량은 무한히 커질수 있다.

5.2 해결책

로그 관리 장치에 로그의 어떤 부분을 안전하게 폐기 할수 있는지 알려주는 메커니즘
  로그를 폐기할수 있는 가장 낮은 오프셋인 로우 워터마크 제공 
  별도의 스레드에서 백그라운드 작업을 실행해 지속적으로 로그의 어떤 부분을 폐기할수 있는지 확읺하고 해당파일을 디스크에서 삭제

5.3 스냅샷 기반 로우 워터마크 

주키퍼나, 래프트 같은 합의 구현체에서 사용
  저장소 엔진은 주기적으로 스냅샷을 생성 
  스냅샷에는 적용에 성공한 로그 인덱스도 함께 저장한다.

5.4 시간 기반 로우 워터마크

일부 시스템에서는 시스템상태를 갱신하는 데 로그를 사용하지 않음 

시스템은 다른 하위시스템이 로그를 제거해도 되는 최저 로그 인덱스를 공유해 주길 기다리지않고 정해진 시간
윈도우가 지나면 로그를 폐기 가능 

로그 크기에 따라 지울수도 있음

5.5 사례

주키퍼,레프트 -> 스냅샷 기반 

아파치 카프카 -> 시간 기반 

6장 리더 팔로워

6.1 문제

데이터를 관리하는 시스템에서 내결합성을 달성하려면 여러서버로 데이터를 복제할 필요가있다.

클라이언트에게 일관성을 어느정도 보장해주는 일도 매우중요. 보다 강한 일관성이 보장되어야한다.

6.2 해결책

클러스터에서 서버 하나를 리더로 선정 
  전체 클러스터를 대표해 결정을 내리고 그 결정을 다른 모든 서버에 전파하는 책임이있다.

모든 서버는 시작시 기존 리더를 찾고 찾지못한다면 리더 선출을 한다.

서버는 리더 선출이 끝나야지 요청을 받고 모든 요청은 오직 리더만 처리

6.3 리더 선출 

리더 선출은 서버를 구동할때 일어 난다.

시스템은 리더가 선출되지 않는한 어떤 클라이언트 요청도 받아들이지 않는다.
  선출 될때마다 세대시계 갱신
  리더, 팔로워, 리더 탐색 상태로만 있는다.

하트비트 는 기존 리더가 실패 했을때를 감지하기 위해 사용

6.3.1 선출 알고리즘

가장 최신 상태인 서버만이 합법적으로 리더가 될수있다.
  최신 세대시계
  쓰기전 로그의 최신 로그 인덱스

모든 서버가 동일하게 최신 상태인경우
  잽 - 일부 구현의 특정 기준에 따라 여러 서버가 더 높은 우선순위를 가지는지 또는 높은 ID를 가지는지
  래프트 - 한번에 서버 하나만 투표요청을 할수 있다면 그중 가장 먼저 선거를 시작한 서버

이미 리더선출을 했다면 동일 세대에서는 리더 선출하지 않음 

서버 과반으로부터 표를 받으면 리더가 되고 리더는 하트비트를 팔로워에게 보낸다.
  팔로워가 지정된 시간 간격내에 하트비트를 받지 못하면 새로운 리더 선출이 발동된다.

6.3.2 일관성 코어를 이용한 리더 선출 

수천대의 노드로 구성된 클러스터는 주키퍼 래프트같이 일관성 코어를 사용하는편이 수월하다.

일관성 코어는 내부적으로 합의를 사용하여 선형성(모든 연산이 하나의 노드에서 수행하는것처럼)을 보장

대규모 클러스터에서는 일반적으로 마스터 또는 컨트롤러 노드로 지정한 서버를 두는데 이 서버는 전체 클러스터를 대신해 결정

리더 선출 구현 필요한 세가지
  키를 원자적으로 실행할수 있는 compareAndSwap 명령어
  선출된 리더에게서 하트비트를 받지 못하면 키를 만료해 새 선거를 발동할수있는 하트비트 구형
  키가 만료되면 관심있은 모든 서버에 알릴수 있는 메커니즘

리더 선출을 하기위해서는 각각의 서버는 compareAndSwap명령어를 사용해 외부저장소에 키를 생성하려한다.
  키를 가장먼저 생성한 서버가 리더로 선출 

래프트는 compare키가 존재하지 않는 경우에만 키 설정 연산 허용

아파치 주키퍼는 예외를 발생시켜서 구현

주키퍼는 일시노드 개념


6.4 정족수 읽기/쓰기가 강한 일관성을 보장하기에 충분하지 않은 이유

저장소 클러스터에서는 한 클라이언트가 특정값을 읽었을때 이후의 모든읽기가 계속해서 동일한 값을 얻게된다는 보장이없다.

7장 하트비트

서버 실패를 적시에 감지하는 일은 매우 중요하다.

서버하나가 실패하면 서버가 담당하는 데이터에 대한 요청을 다른 서버가 담당해 처리하도록 수정해야하기 때문에

7.1 해결책

다른 모든 서버에 주기적으로 요청을 보내 송신 서버의 활동성을 알린다.

요청시간 간격을 서버사이의 네트워크 왕복 시간 보다 길게 설정
  모든 수신서버는 요청시간 간격의 배수인 타임아웃 간격 동안 기다린다.

타임아웃 간격 > 요청 시간 간격 > 서버간 네트워크 왕복 시간

하트비트 시간 간격 및 타임아웃 값을 선택할때 네트워크 왕복시간을 파악해두는것이 매우 도움이된다.

실패감지자 구현
  수신서버가 하트비트를 받을때 마다 호출하는 메서드 이 메서드가 호출하되면 실패감지자에게 이상없음을 알린다.
  주기적으로 하트비트 상태를 검사하고 실패가능성을 감지하는 메서드

하트비트 간격이 짧을수록 실패감지는 빠르지만 가짜 실패 확률이 높아진다.
  하트비트 간격은 클러스터의 요구사항에 따라 구현된다.

7.2 소규모 클러스터: 합의기반 시스템

모든 합의 시스템 구현에서 하트비트는 리더 서버에서 전체 팔로워로 전송
  하트비트를 수신할때마다 하트비트가 도착한 시간의 타임스탬프를 기록한다.

고정시간 윈도우내에 하트비트를 수신하지 못하면 새로운 리더 선출 
  이과정에서 프로세스들이 느리거나 네트워크 지연으로 인해 가짜 실패를 감지할 가능성이있다.
  예전 리더인지 감지하기위해 세대시계를 사용해야한다.

7.3 기술적 고려사항

단일 소켓 채널을 사용해 서버사이에서 통신할때 HOL블로킹이 하트비트 메세지를 방해하지 않도록 해야한다.

단일 갱신 큐를 사용하면 디스크쓰기 같은 일부 작업이 타이밍 인터럽트처리와 하트비트 전송에 지연을 초래할 수있다.

하트비트를 비동기로 전송하는 별도의 스레드를 사용하는 방법으로 해결 
  하시코프 컨슬, 아키 같은 프레임워크 사용

7.4 대규모 클러스터 : 가십기반 프로토콜

고려사항
  각각의 서버는 고정된 수 이상의 메시지를 생성하지 않아야 한다.
  하트비트 메시지가 사용하는 총 대역폭이 너무 많은 대역폭을 차지 하지 않아야한다.

대규모 클러스터는 올투올 하트비트 방식은 사용하지않음 

가십 전파와 실패감지자를 함께 사용한다.

가십전파 프로토콜은 실패정보를 클러스터 전체에 전파하기 위해 사용한다.
  대규모 클러스터는 실패시 노드간 데이터 이동이 발생하므로 정확한 실패 감지를 위한 상대적으로 더긴 지연을 허용하는 방식사용
  네트워크 지연과 느린처리속도록 인해 정상 노드를 실패로 감지하지 않아야한다.

일반적인 메커니즘 각각의 프로세스에 의심번호 부여 
  제한된 시간내에 해당 프로세스를 포함한 가십이 없으면 의심번호 증가 
  의심번호는 과거 통계를 기반으로 계산 

두가지 주요구현
  아카와 카산드라에서 사용하는 파이축적
    실패감지를 위해 의심수준을 점진적으로 쌓아가는 방식 네트워크 상태와 노드의 과거 동작패턴을 고려해 판단 고정된 타임아웃대신 네트워크 상태에따라 동적으로 조정되는 임곗값사용
  하시코프 컨슬과 멤버리스트에서 사용하는 라이프가드 증강을 적용한 SWIM
    대규모 분산 시스템에서 노드들의 멤버십을 효율적으로 관리하고 장애를 감지하기위한 프로토콜

6장 과반수 정족수 

6.1 문제

분산 시스템에서 서버가 어떤 동작을 수행할 때 는 해당 서버가 죽더라도 그결과를 클리어언트에게 제공하도록 보장(복제로 달성 )

의문 - 원본서버가 갱신을 완전히 적용했음을 확신하려면 몇대의 서버에 복제가 완료?
  너무 많은서버를 기다리면 응답이 느려짐
  충분히 복제가 이루어지지 않으면 갱신 손실이 발생함

즉 전체 시스템 성능과 시스템 무결성간에 균형을 맞추는 일이 중요

6.2 해결책

정족수
  클러스터는 클러스터내 과반수 노드가 갱신을 확인하면 갱신을 수신했다고 간주
  필요성은 얼마나 많은 실패를 허용할 수 있는지를 가리킨다.

필요한 두가지 예제
  클러스터 내 여러 서버에서 데이터를 갱신할때 - 클라이언트에게 과반수 서버에서 사용 가능하다고 보장한 데이터만 
  볼수있게 하는 하이 워터마크 사용
  리더 선출 - 리더 팔로워에서 과반수 서버로부터 표를 받은 경우에만 선택된다.

6.2.1 클러스터내 서버 수 결정하기

클러스터는 정족수의 서버가 가동중일때만 제기능을 수행한다.

두가지 고려사항
  쓰기 연산의 처리량 
    데이터 쓰기의 지연시간은 정족수를 형성하는 서버 수에 정비례한다.
    클러스터 수의 서버를 두배로 늘리면 원래 클러스터의 처리량이 절반으로 줄어든다.
  허용 가능한 실패수
    허용가능한 실패수는 클러스터 크기에 따라 달라진다.
    하지만 기존 클러스터에 서버 한대를 추가한다고 해서 내결함성이 항상 높아지지않는다.

두가지 요인을 보면 대다수 정족수 기반 시스템은 클러스터의 크기가 3 또는 5다.

6.2.2 탄력적 정족수

과반수 정족수에서 서로 다른 정족수 둘은 적어도 하나의 노드에서 항상 겹친다.

핵심은 이정족수의 교집합에 있다.
  여러 연산이 서로 다른 정족수 크기를 사용하더라도 교집합이 있다면 연산 수행에 문제없음 
  서로 다른 정족수 크기를 사용하는 주된 이점은 빈번한 연산에도 상대적으로 작은 크기의 정족수를 사용할 수 있다는 점이다.

정족수 교집합은 두단계에 걸쳐 실행이 필요하다.
  복제로그 의 일반적인 구현에서 1단계는 리더 선출 관여하는데 빈도는 낮은편이므로 이단게에서는 더큰 정족수 사용가능 
  그외는 1단계보다 작은 정족수 크기를 사용해 전체 처리량과 지연시간을 개선 가능 

6.3 사례

모든 합의 구현체는 과반수 정족수를 사용(잽, 래프트, 팍소스)

7장 세대 시계

7.1 문제

리더 팔로워 구성에서 일시적으로 리더와 팔로워의 연결이 끊어질 가능성이 있다.
이런 경우 리더 프로세스가 동작중이므로 복구될때 팔로워에게 복제 요청을 보내려 할것이다.

클러스터의 나머지 노드가 원래의 리더가 보낸 요청을 감지하는것이 중요

이전 리더 스스로도 일시적으로 클러스터와 연결이 끊어졌음을 감지하고 리더십에게 물러나기위해 필요한 조치를 취해야한다.

7.2 해결책 

램포트 시계
  서버 세대를 나타내는 단조적으로 증가 하는 숫자를 유지관리한다.
  새 리더 선출이있을때마다 세대 증가 
  리부팅 이후에도 유지되어야 하므로 쓰기전 로그의 모든 엔트리와 함께저장
  팔로워는 하이 워터마크를 정보를 통해 로그에서 충돌한 엔트리를 찾는다.

1. 서버가 시작될때 서버는 로그에서 알려진 최신세대를 읽음 
2. 리더 팔로워 패턴에서 서버들은 새리더 선출이 있을때마다 세대 증가
3. 서버는 세대 정보를 투표요청에 포함시켜 다른서버에 전송 및 성공적으로 리더선출후 모두 동일한 세대를 갖게 된다. 리더가 선출시 팔로워에게 새로운 세대를 알린다.

모든 하트비트 메세지와 팔로워에게 보내는 복제 요청에도 세대를 포함 리더는 쓰기전 로그에 기록하는 모든 엔트리에 세대 정보를 함께 기록

만약 팔로워가 퇴출된 리더에게 메시지를 받으면 세댓값이 낮아 실패응답을 받는다.

7.3 사례

래프트는 리더 세대를 표시하기 위해 텀개념 사용

주키퍼는 에폭 번호를 트랜잭션 ID일부분으로 기록
  모든 트랜잭션에는 에폭으로 표시된 세대가 포함

아파치 카산드라는 서버가 재시작할때마다 증가하는 세대번호를 저장 
  세대 정보는 키스페이스에 기록하고 가십메시지로 다른서버로 전파
  자신이 알고있는 세댓값과 메시지의 세댓값을 비교

아파치 카프카 
  새 컨트롤러를 선출 할때마다 에폭 번호를 생성해 주키퍼에 저장

8장 하이 워터마크 

8.1 문제

리더가 로그 엔트리를 팔로워에게 보내기 전에 실패할 수 있다.

리더가 일부 팔로워에게 로그 엔트리를 보냈지만 과반수에 미치기 전에 실패 할수 있다.

각각의 팔로워는 클라이언트에게 로그의 어떤 부분까지를 안전하게 제공할수 있는지 아는것이 중요하다.

8.2 해결책 

하이워터마크는 로그 파일의 인덱스로, 과반수 정족수에 해당하는 팔로워에 성공적으로 복제된 마지막 로그 엔트리를 기록한다.

리더는 팔로워에 복제를 실행하는 과정에서 하이워터마크를 전달 , 클러스터의 모든서버는 하이 워터마크 이하에 있는 갱신 데이터만 
클라이언트에게 전종해야함.

팔로워는 복제 요청을 처리하고 로그 엔트리를 자신의 로컬로그에 덧붙이는데 성공적으로 덧붙이면 가장 최신로그 엔트리의 인덱스와 함께
리더에게 응답(현재 세대시계 포함)

하이워터마크는 모든 팔로워의 로그인덱스와 리더 자체의 로그를 살펴보고 계산
서버 과반이상에서 사용할수 있는 인덱스를 골라 하이워터마크로 사용

리더는 정기적으로 전송하는 하트비트의 일부 또는 별도의 요청으로 하이워터마크를 팔로워에게 전파

클라이언트는 하이워터마크까지만 로그 엔트리를 읽을수있다.

리더가 실패하고 다른 서버가 리더 선출이되면 하이 워터마크 이상의 엔트리는 사용불가

8.2.1 로그 절단

서버가 죽은후 클러스터에 재합류시 로그에서 충돌하는 엔트리가 남아있을 가느엇ㅇ이 있다.

따라서 서버가 클러스터에 합류 할때 마다 로그의 어느 엔트리가 잠재적으로 충돌하는지 리더에게 확인 

그럼다음 리더와 일치하는 지점까지 로그를 절단하고 후속 엔트리를 갱신해 자신의 로그를 클러스터 나머지 서버와 일치시킨다.

서버가 죽은후 재합류시 최신리더를 찾는다. 그리고 현재의 하이워터마크를 명시적으로 요청하고 자신의 로그를 하이워터마크 까지 절단후 
리더에게서 하이워터마크 이후의 모든 엔트리를 받아온다.

래프트 같은 복제 알고리즘에는 요청 로그 엔트리와 보유 로그 엔트리를 대조해 충돌엔트리를 찾는방법이있음
  동일한 로그 인덱스지만 세대 시계 값이 낮은 엔트리는 제거

8.3 사례

모든 합의 알고리즘은 요청된 상태 변경을 언제 적용할지 알기위해 하이워터마크 개념사용 

래프트 - 커밋인덱스라고 부름 

아파치 카프카 - 별도 인덱스로 유지 관리하는 하이 워터마크가 있음 
  컨슈머는 하이 워터마크까지의 엔트리만 볼수있음
  




